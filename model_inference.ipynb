{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50484d2-bfa4-40b6-82cd-e8bf86187cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pprint\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType, DateType\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score, roc_auc_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcac9436-de95-4ebf-9655-56df8e1eafd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a .py script that takes a snapshot date, loads a model artefact and make an inference and save to datamart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c91bb1-bcf0-4195-90f3-dc88806ebf8c",
   "metadata": {},
   "source": [
    "## set up pyspark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32fb3bc6-4166-4893-88e1-0d3140df5a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:18:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Initialize SparkSession\n",
    "spark = pyspark.sql.SparkSession.builder \\\n",
    "    .appName(\"gold_model_prediction\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to ERROR to hide warnings\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30206071-5f00-4c3b-be13-55c54db8e336",
   "metadata": {},
   "source": [
    "## set up config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca7d9f0-cfbc-4098-826c-5537ba56b108",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_date_str = \"2024-06-01\"\n",
    "model_name = \"xgb_model_pipeline.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75f0bb22-745b-4342-9779-4425795dc752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_artefact_filepath': 'model_artifacts/xgb_churn_model.joblib',\n",
      " 'model_directory': 'model_artifacts/',\n",
      " 'model_name': 'xgb_churn_model.joblib',\n",
      " 'snapshot_date': datetime.datetime(2024, 6, 1, 0, 0),\n",
      " 'snapshot_date_str': '2024-06-01'}\n"
     ]
    }
   ],
   "source": [
    "config = {}\n",
    "config[\"snapshot_date_str\"] = snapshot_date_str\n",
    "config[\"snapshot_date\"] = datetime.strptime(config[\"snapshot_date_str\"], \"%Y-%m-%d\")\n",
    "config[\"model_name\"] = model_name\n",
    "config[\"model_directory\"] = \"model_artifacts/\"\n",
    "config[\"model_artefact_filepath\"] = config[\"model_directory\"] + config[\"model_name\"]\n",
    "\n",
    "pprint.pprint(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea8c974-7a80-44ec-a73f-b72c46b70972",
   "metadata": {},
   "source": [
    "## load model from model artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4704571-1729-49ef-b2fb-e7346fc37d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully! model_artifacts/xgb_churn_model.joblib\n"
     ]
    }
   ],
   "source": [
    "with open(config[\"model_artefact_filepath\"], 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "\n",
    "preprocessor = obj['preprocessor']\n",
    "model = obj['model']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441303bb-1736-4589-8537-c914d8d843b1",
   "metadata": {},
   "source": [
    "## load feature store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d002feb1-30f5-415a-91ef-b686ee8de99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracted features_sdf 6948 2024-06-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "parquet_path = f\"datamart/gold/feature_store/gold_feature_store_{config['snapshot_date_str']}.parquet\"\n",
    "features_store_sdf = spark.read.parquet(parquet_path)\n",
    "\n",
    "# extract feature store\n",
    "features_sdf = features_store_sdf.filter((col(\"snapshot_date\") == config[\"snapshot_date\"]))\n",
    "print(\"extracted features_sdf\", features_sdf.count(), config[\"snapshot_date\"])\n",
    "\n",
    "# Extract IDs before dropping them\n",
    "id_cols_pdf = features_sdf.select(\"customerID\", \"snapshot_date\").toPandas()\n",
    "\n",
    "features_pdf = features_sdf.toPandas()\n",
    "features_pdf.drop(columns=[c for c in [\"customerID\", \"snapshot_date\"] if c in features_pdf.columns], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f361665-930e-47ec-b312-679ecd40cb2e",
   "metadata": {},
   "source": [
    "## preprocess data for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3b44009-0ae7-41ca-a073-28bcbc23c8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop identifiers \n",
    "features_pdf.drop(columns=[c for c in [\"customerID\", \"snapshot_date\"] if c in features_pdf.columns], inplace=True)\n",
    "\n",
    "# Recreate tenure groups\n",
    "def create_tenure_groups(df):\n",
    "    df = df.copy()\n",
    "    df[\"tenure_group\"] = pd.cut(\n",
    "        df[\"tenure\"],\n",
    "        bins=[0, 12, 24, 36, 48, 60, 72, np.inf],\n",
    "        labels=[\"0-1yr\", \"1-2yr\", \"2-3yr\", \"3-4yr\", \"4-5yr\", \"5-6yr\", \"6+yr\"]\n",
    "    )\n",
    "    return df\n",
    "\n",
    "features_pdf = create_tenure_groups(features_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4045799d-d9cb-404f-99b3-f63f5499318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "numerical_cols = features_pdf.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "categorical_cols = features_pdf.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "\n",
    "for num_col in numerical_cols:\n",
    "    median_val = features_pdf[num_col].median()\n",
    "    features_pdf[num_col] = features_pdf[num_col].fillna(median_val)\n",
    "\n",
    "for cat_col in categorical_cols:\n",
    "    if pd.api.types.is_categorical_dtype(features_pdf[cat_col]):\n",
    "        if \"Unknown\" not in features_pdf[cat_col].cat.categories:\n",
    "            features_pdf[cat_col] = features_pdf[cat_col].cat.add_categories([\"Unknown\"])\n",
    "    mode_val = features_pdf[cat_col].mode()[0] if not features_pdf[cat_col].mode().empty else \"Unknown\"\n",
    "    features_pdf[cat_col] = features_pdf[cat_col].fillna(mode_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a4362f-9dee-4838-a030-a74b88884b4f",
   "metadata": {},
   "source": [
    "## model prediction inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "517a5173-a9b5-44ac-b210-fdc8376949ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform and predict\n",
    "X_inference = preprocessor.transform(features_pdf)\n",
    "y_inference = model.predict(X_inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66bf1150-721f-43ca-bba9-622f9383ffac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   customerID snapshot_date              model_name  model_predictions\n",
      "0  7590-VHVEG    2024-06-01  xgb_churn_model.joblib                  0\n",
      "1  5575-GNVDE    2024-06-01  xgb_churn_model.joblib                  0\n",
      "2  3668-QPYBK    2024-06-01  xgb_churn_model.joblib                  0\n",
      "3  7795-CFOCW    2024-06-01  xgb_churn_model.joblib                  0\n",
      "4  9237-HQITU    2024-06-01  xgb_churn_model.joblib                  0\n",
      "5  9305-CDSKC    2024-06-01  xgb_churn_model.joblib                  1\n",
      "6  1452-KIOVK    2024-06-01  xgb_churn_model.joblib                  0\n",
      "7  6713-OKOMC    2024-06-01  xgb_churn_model.joblib                  0\n",
      "8  7892-POOKP    2024-06-01  xgb_churn_model.joblib                  0\n",
      "9  6388-TABGU    2024-06-01  xgb_churn_model.joblib                  0\n",
      "Number of predictions: 6948\n"
     ]
    }
   ],
   "source": [
    "y_inference_pdf = id_cols_pdf.copy()\n",
    "y_inference_pdf[\"model_name\"] = config[\"model_name\"]\n",
    "y_inference_pdf[\"model_predictions\"] = y_inference\n",
    "\n",
    "print(y_inference_pdf.head(10))\n",
    "print(\"Number of predictions:\", len(y_inference_pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3fe2b8-4642-486d-aa3b-2d7703ad3d15",
   "metadata": {},
   "source": [
    "## save model inference to datamartgold table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b6ea81-a97b-423a-bf95-4c8072088c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved inference results to: datamart/gold/model_predictions/xgb_churn_model.jo/xgb_churn_model.jo_predictions_2024_06_01.parquet\n"
     ]
    }
   ],
   "source": [
    "gold_directory = f\"datamart/gold/model_predictions/{config['model_name'][:-4]}/\"\n",
    "os.makedirs(gold_directory, exist_ok=True)\n",
    "\n",
    "partition_name = f\"{config['model_name'][:-4]}_predictions_{config['snapshot_date_str'].replace('-', '_')}.parquet\"\n",
    "filepath = os.path.join(gold_directory, partition_name)\n",
    "\n",
    "# Save Parquet\n",
    "spark.createDataFrame(y_inference_pdf).write.mode(\"overwrite\").parquet(filepath)\n",
    "print(\"Saved inference results to:\", filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4947d9dc-98ac-421f-955c-2d4b741343d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved inference results to CSV: datamart/gold/model_predictions/xgb_churn_model.jo/xgb_churn_model.jo_predictions_2024_06_01.csv\n"
     ]
    }
   ],
   "source": [
    "# Save CSV\n",
    "csv_filepath = os.path.join(gold_directory, partition_name.replace(\".parquet\", \".csv\"))\n",
    "y_inference_pdf.to_csv(csv_filepath, index=False)\n",
    "print(\"Saved inference results to CSV:\", csv_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c339e3cb-1826-49a0-ac73-cb381f85b033",
   "metadata": {},
   "source": [
    "## Check datamart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa5b3c88-bb72-4649-aa47-89b59dd6924b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session \n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"xgb_model_checker\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")  # Clean output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9182b2ec-8daa-461c-b301-2e3c0f0f9e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read Parquet files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row_count: 114392\n",
      "+----------+-------------+--------------------+-----------------+\n",
      "|customerID|snapshot_date|          model_name|model_predictions|\n",
      "+----------+-------------+--------------------+-----------------+\n",
      "|4526-ZJJTM|   2024-07-01|log_reg_churn_mod...|                0|\n",
      "|8384-FZBJK|   2024-07-01|log_reg_churn_mod...|                0|\n",
      "|3750-RNQKR|   2024-07-01|log_reg_churn_mod...|                0|\n",
      "|0962-CQPWQ|   2024-07-01|log_reg_churn_mod...|                0|\n",
      "|3096-YXENJ|   2024-07-01|log_reg_churn_mod...|                0|\n",
      "+----------+-------------+--------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"datamart/gold/model_predictions/\"\n",
    "\n",
    "# Recursively list all files in the subfolders of the folder_path\n",
    "files_list = glob.glob(os.path.join(folder_path, '**', '*'), recursive=True)\n",
    "\n",
    "# Filter out only Parquet and CSV files\n",
    "parquet_files = [f for f in files_list if f.endswith(\".parquet\")]\n",
    "csv_files = [f for f in files_list if f.endswith(\".csv\")]\n",
    "\n",
    "# Read the Parquet files if they exist\n",
    "if parquet_files:\n",
    "    df = spark.read.option(\"header\", \"true\").parquet(*parquet_files)\n",
    "    print(\"Read Parquet files\")\n",
    "elif csv_files:\n",
    "    # Read the CSV files if they exist\n",
    "    df = spark.read.option(\"header\", \"true\").csv(*csv_files)\n",
    "    print(\"Read CSV files\")\n",
    "else:\n",
    "    print(\"No valid Parquet or CSV files found.\")\n",
    "\n",
    "# Show row count and schema of the DataFrame\n",
    "print(\"row_count:\", df.count())\n",
    "df.show(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff19c58-94ad-4abb-984f-b082326c2871",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
