{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e21a4f4b-e904-42af-832c-d32585219208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install model_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b50484d2-bfa4-40b6-82cd-e8bf86187cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pprint\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType, DateType\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score, roc_auc_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import joblib\n",
    "from joblib import load\n",
    "# import model_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcac9436-de95-4ebf-9655-56df8e1eafd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a .py script that takes a snapshot date, loads a model artefact and make an inference and save to datamart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c91bb1-bcf0-4195-90f3-dc88806ebf8c",
   "metadata": {},
   "source": [
    "## set up pyspark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32fb3bc6-4166-4893-88e1-0d3140df5a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/24 06:45:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/06/24 06:45:07 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Initialize SparkSession\n",
    "spark = pyspark.sql.SparkSession.builder \\\n",
    "    .appName(\"gold_model_prediction\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to ERROR to hide warnings\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30206071-5f00-4c3b-be13-55c54db8e336",
   "metadata": {},
   "source": [
    "## set up config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ca7d9f0-cfbc-4098-826c-5537ba56b108",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_date_str = \"2024-07-01\"\n",
    "model_name = \"log_reg_churn_model.joblib\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75f0bb22-745b-4342-9779-4425795dc752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_artefact_filepath': 'model_artifacts/log_reg_churn_model.joblib',\n",
      " 'model_directory': 'model_artifacts/',\n",
      " 'model_name': 'log_reg_churn_model.joblib',\n",
      " 'snapshot_date': datetime.datetime(2024, 7, 1, 0, 0),\n",
      " 'snapshot_date_str': '2024-07-01'}\n"
     ]
    }
   ],
   "source": [
    "config = {}\n",
    "config[\"snapshot_date_str\"] = snapshot_date_str\n",
    "config[\"snapshot_date\"] = datetime.strptime(config[\"snapshot_date_str\"], \"%Y-%m-%d\")\n",
    "config[\"model_name\"] = model_name\n",
    "config[\"model_directory\"] = \"model_artifacts/\"\n",
    "config[\"model_artefact_filepath\"] = config[\"model_directory\"] + config[\"model_name\"]\n",
    "\n",
    "pprint.pprint(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea8c974-7a80-44ec-a73f-b72c46b70972",
   "metadata": {},
   "source": [
    "## load model from model artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4704571-1729-49ef-b2fb-e7346fc37d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully! model_artifacts/log_reg_churn_model.joblib\n"
     ]
    }
   ],
   "source": [
    "# Load the model from the .joblib file\n",
    "with open(config[\"model_artefact_filepath\"], 'rb') as file:\n",
    "    model_artefact = joblib.load(file)\n",
    "\n",
    "print(\"Model loaded successfully! \" + config[\"model_artefact_filepath\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441303bb-1736-4589-8537-c914d8d843b1",
   "metadata": {},
   "source": [
    "## load feature store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d002feb1-30f5-415a-91ef-b686ee8de99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features_sdf: 25124 2024-04-01 00:00:00 2024-07-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "gold_feature_store_path = \"datamart/gold/feature_store/\"\n",
    "\n",
    "config[\"train_test_start_date_str\"] = \"2024-04-01\"\n",
    "config[\"oot_end_date_str\"] = \"2024-07-01\"\n",
    "config[\"train_test_start_date\"] = datetime.strptime(config[\"train_test_start_date_str\"], \"%Y-%m-%d\")\n",
    "config[\"oot_end_date\"] = datetime.strptime(config[\"oot_end_date_str\"], \"%Y-%m-%d\")\n",
    "\n",
    "available_files = os.listdir(gold_feature_store_path)\n",
    "target_dates = [\"2024-04-01\", \"2024-05-01\", \"2024-06-01\", \"2024-07-01\"]\n",
    "\n",
    "target_files = [\n",
    "    os.path.join(gold_feature_store_path, f\"gold_feature_store_{d}.parquet\")\n",
    "    for d in target_dates if f\"gold_feature_store_{d}.parquet\" in available_files\n",
    "]\n",
    "\n",
    "if not target_files:\n",
    "    raise FileNotFoundError(\"No matching Parquet files found for the given date range.\")\n",
    "\n",
    "features_sdf = spark.read.parquet(*target_files)\n",
    "features_sdf = features_sdf.filter(\n",
    "    (col(\"snapshot_date\") >= config[\"train_test_start_date\"]) &\n",
    "    (col(\"snapshot_date\") <= config[\"oot_end_date\"])\n",
    ")\n",
    "\n",
    "print(\"Extracted features_sdf:\", features_sdf.count(), config[\"train_test_start_date\"], config[\"oot_end_date\"])\n",
    "\n",
    "# Extract IDs before dropping them\n",
    "id_cols_pdf = features_sdf.select(\"customerID\", \"snapshot_date\").toPandas()\n",
    "\n",
    "features_pdf = features_sdf.toPandas()\n",
    "features_pdf.drop(columns=[c for c in [\"customerID\", \"snapshot_date\"] if c in features_pdf.columns], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f361665-930e-47ec-b312-679ecd40cb2e",
   "metadata": {},
   "source": [
    "## preprocess data for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a25dd874-6edd-4598-a11f-ccd30c0c3e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(model_artefact))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3b44009-0ae7-41ca-a073-28bcbc23c8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop identifiers \n",
    "features_pdf.drop(columns=[c for c in [\"customerID\", \"snapshot_date\"] if c in features_pdf.columns], inplace=True)\n",
    "\n",
    "# Recreate tenure groups\n",
    "def create_tenure_groups(df):\n",
    "    df = df.copy()\n",
    "    df[\"tenure_group\"] = pd.cut(\n",
    "        df[\"tenure\"],\n",
    "        bins=[0, 12, 24, 36, 48, 60, 72, np.inf],\n",
    "        labels=[\"0-1yr\", \"1-2yr\", \"2-3yr\", \"3-4yr\", \"4-5yr\", \"5-6yr\", \"6+yr\"]\n",
    "    )\n",
    "    return df\n",
    "\n",
    "features_pdf = create_tenure_groups(features_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4045799d-d9cb-404f-99b3-f63f5499318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "numerical_cols = features_pdf.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "categorical_cols = features_pdf.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "\n",
    "for col in numerical_cols:\n",
    "    median_val = features_pdf[col].median()\n",
    "    features_pdf[col] = features_pdf[col].fillna(median_val)\n",
    "\n",
    "for col in categorical_cols:\n",
    "    mode_val = features_pdf[col].mode()[0] if not features_pdf[col].mode().empty else \"Unknown\"\n",
    "    features_pdf[col] = features_pdf[col].fillna(mode_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1d7596-5f87-4f77-81a5-18947af013ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1a4362f-9dee-4838-a030-a74b88884b4f",
   "metadata": {},
   "source": [
    "## model prediction inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ec78728-99af-4b99-89db-ac7f14da5385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and preprocessor \n",
    "pipeline = load(\"model_artifacts/model_pipeline.pkl\")\n",
    "preprocessor = pipeline[\"preprocessor\"]\n",
    "model = pipeline[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "517a5173-a9b5-44ac-b210-fdc8376949ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform and predict\n",
    "X_inference = preprocessor.transform(features_pdf)\n",
    "y_inference = model.predict(X_inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66bf1150-721f-43ca-bba9-622f9383ffac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   customerID snapshot_date                  model_name  model_predictions\n",
      "0  7590-VHVEG    2024-06-01  log_reg_churn_model.joblib                  0\n",
      "1  5575-GNVDE    2024-06-01  log_reg_churn_model.joblib                  0\n",
      "2  3668-QPYBK    2024-06-01  log_reg_churn_model.joblib                  0\n",
      "3  7795-CFOCW    2024-06-01  log_reg_churn_model.joblib                  0\n",
      "4  9237-HQITU    2024-06-01  log_reg_churn_model.joblib                  0\n",
      "Number of predictions: 25124\n"
     ]
    }
   ],
   "source": [
    "y_inference_pdf = id_cols_pdf.copy()\n",
    "y_inference_pdf[\"model_name\"] = config[\"model_name\"]\n",
    "y_inference_pdf[\"model_predictions\"] = y_inference\n",
    "\n",
    "print(y_inference_pdf.head())\n",
    "print(\"Number of predictions:\", len(y_inference_pdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7792be8f-7a05-4318-a16a-54f2375299df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf3fe2b8-4642-486d-aa3b-2d7703ad3d15",
   "metadata": {},
   "source": [
    "## save model inference to datamartgold table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62b6ea81-a97b-423a-bf95-4c8072088c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved inference results to: datamart/gold/model_predictions/log_reg_churn_model.jo/log_reg_churn_model.jo_predictions_2024_04_01_to_2024_07_01.parquet\n"
     ]
    }
   ],
   "source": [
    "snapshot_range_str = f\"{config['train_test_start_date_str']}_to_{config['oot_end_date_str']}\".replace(\"-\", \"_\")\n",
    "gold_directory = f\"datamart/gold/model_predictions/{config['model_name'][:-4]}/\"\n",
    "\n",
    "if not os.path.exists(gold_directory):\n",
    "    os.makedirs(gold_directory)\n",
    "\n",
    "partition_name = f\"{config['model_name'][:-4]}_predictions_{snapshot_range_str}.parquet\"\n",
    "filepath = os.path.join(gold_directory, partition_name)\n",
    "\n",
    "spark.createDataFrame(y_inference_pdf).write.mode(\"overwrite\").parquet(filepath)\n",
    "print(\"Saved inference results to:\", filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4947d9dc-98ac-421f-955c-2d4b741343d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved inference results to CSV: datamart/gold/model_predictions/log_reg_churn_model.jo/log_reg_churn_model.jo_predictions_2024_04_01_to_2024_07_01.csv\n"
     ]
    }
   ],
   "source": [
    "# Save as CSV\n",
    "csv_filepath = filepath.replace(\".parquet\", \".csv\")  # Change the extension to .csv\n",
    "\n",
    "# Save the predictions as CSV using pandas \n",
    "y_inference_pdf.to_csv(csv_filepath, index=False)\n",
    "print(\"Saved inference results to CSV:\", csv_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c339e3cb-1826-49a0-ac73-cb381f85b033",
   "metadata": {},
   "source": [
    "## Check datamart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9182b2ec-8daa-461c-b301-2e3c0f0f9e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 25124 rows from:\n",
      " - datamart/gold/model_predictions/log_reg_churn_model.jo/log_reg_churn_model.jo_predictions_2024_04_01_to_2024_07_01.parquet\n",
      "+----------+-------------+--------------------+-----------------+\n",
      "|customerID|snapshot_date|          model_name|model_predictions|\n",
      "+----------+-------------+--------------------+-----------------+\n",
      "|4526-ZJJTM|   2024-07-01|log_reg_churn_mod...|                0|\n",
      "|8384-FZBJK|   2024-07-01|log_reg_churn_mod...|                0|\n",
      "|3750-RNQKR|   2024-07-01|log_reg_churn_mod...|                0|\n",
      "|0962-CQPWQ|   2024-07-01|log_reg_churn_mod...|                0|\n",
      "|3096-YXENJ|   2024-07-01|log_reg_churn_mod...|                0|\n",
      "|1265-BCFEO|   2024-07-01|log_reg_churn_mod...|                0|\n",
      "|5837-LXSDN|   2024-07-01|log_reg_churn_mod...|                0|\n",
      "|5945-AZYHT|   2024-07-01|log_reg_churn_mod...|                0|\n",
      "|8325-QRPZR|   2024-07-01|log_reg_churn_mod...|                0|\n",
      "|6384-VMJHP|   2024-07-01|log_reg_churn_mod...|                0|\n",
      "+----------+-------------+--------------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- customerID: string (nullable = true)\n",
      " |-- snapshot_date: date (nullable = true)\n",
      " |-- model_name: string (nullable = true)\n",
      " |-- model_predictions: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session \n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"lr_model_checker\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")  # Clean output\n",
    "\n",
    "# Define model folder and prediction file name pattern \n",
    "model_name = \"log_reg_churn_model.jo\"\n",
    "snapshot_range = \"2024_04_01_to_2024_07_01\"\n",
    "\n",
    "folder_path = f\"datamart/gold/model_predictions/{model_name}/\"\n",
    "\n",
    "# Find matching Parquet files\n",
    "files_list = [\n",
    "    f for f in glob.glob(os.path.join(folder_path, \"*.parquet\"))\n",
    "    if snapshot_range in os.path.basename(f)\n",
    "]\n",
    "\n",
    "if not files_list:\n",
    "    print(\"No matching prediction files found for:\", snapshot_range)\n",
    "    print(\"Available files in folder:\", os.listdir(folder_path))\n",
    "else:\n",
    "    # Load data into Spark DataFrame and inspect\n",
    "    df = spark.read.option(\"header\", \"true\").parquet(*files_list)\n",
    "    print(f\"Loaded {df.count()} rows from:\")\n",
    "    for f in files_list:\n",
    "        print(\" -\", f)\n",
    "    \n",
    "    df.show(10)\n",
    "    df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff19c58-94ad-4abb-984f-b082326c2871",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
