{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b50484d2-bfa4-40b6-82cd-e8bf86187cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pprint\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType, DateType\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score, roc_auc_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import joblib\n",
    "from joblib import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcac9436-de95-4ebf-9655-56df8e1eafd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a .py script that takes a snapshot date, loads a model artefact and make an inference and save to datamart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c91bb1-bcf0-4195-90f3-dc88806ebf8c",
   "metadata": {},
   "source": [
    "## set up pyspark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32fb3bc6-4166-4893-88e1-0d3140df5a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:44:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/06/25 06:44:03 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Initialize SparkSession\n",
    "spark = pyspark.sql.SparkSession.builder \\\n",
    "    .appName(\"gold_model_prediction\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to ERROR to hide warnings\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30206071-5f00-4c3b-be13-55c54db8e336",
   "metadata": {},
   "source": [
    "## set up config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ca7d9f0-cfbc-4098-826c-5537ba56b108",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_date_str = \"2024-06-01\"\n",
    "model_name = \"log_reg_churn_model.joblib\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75f0bb22-745b-4342-9779-4425795dc752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_artefact_filepath': 'model_artifacts/log_reg_churn_model.joblib',\n",
      " 'model_directory': 'model_artifacts/',\n",
      " 'model_name': 'log_reg_churn_model.joblib',\n",
      " 'snapshot_date': datetime.datetime(2024, 6, 1, 0, 0),\n",
      " 'snapshot_date_str': '2024-06-01'}\n"
     ]
    }
   ],
   "source": [
    "config = {}\n",
    "config[\"snapshot_date_str\"] = snapshot_date_str\n",
    "config[\"snapshot_date\"] = datetime.strptime(config[\"snapshot_date_str\"], \"%Y-%m-%d\")\n",
    "config[\"model_name\"] = model_name\n",
    "config[\"model_directory\"] = \"model_artifacts/\"\n",
    "config[\"model_artefact_filepath\"] = config[\"model_directory\"] + config[\"model_name\"]\n",
    "\n",
    "pprint.pprint(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea8c974-7a80-44ec-a73f-b72c46b70972",
   "metadata": {},
   "source": [
    "## load model from model artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4704571-1729-49ef-b2fb-e7346fc37d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully! model_artifacts/log_reg_churn_model.joblib\n"
     ]
    }
   ],
   "source": [
    "# Load the model from the .joblib file\n",
    "with open(config[\"model_artefact_filepath\"], 'rb') as file:\n",
    "    model_artefact = joblib.load(file)\n",
    "\n",
    "print(\"Model loaded successfully! \" + config[\"model_artefact_filepath\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2176260-49d1-426d-92ad-479d913fb8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the preprocessor from the model_artifacts directory\n",
    "preprocessor_filepath = \"model_artifacts/preprocessor.joblib\"\n",
    "with open(preprocessor_filepath, 'rb') as file:\n",
    "    preprocessor = joblib.load(file)\n",
    "\n",
    "print(\"Scaler loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441303bb-1736-4589-8537-c914d8d843b1",
   "metadata": {},
   "source": [
    "## load feature store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d002feb1-30f5-415a-91ef-b686ee8de99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracted features_sdf 6948 2024-06-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "features_store_sdf = spark.read.parquet(\"datamart/gold/feature_store/gold_feature_store_2024-06-01.parquet\")\n",
    "\n",
    "# extract feature store\n",
    "features_sdf = features_store_sdf.filter((col(\"snapshot_date\") == config[\"snapshot_date\"]))\n",
    "print(\"extracted features_sdf\", features_sdf.count(), config[\"snapshot_date\"])\n",
    "\n",
    "# Extract IDs before dropping them\n",
    "id_cols_pdf = features_sdf.select(\"customerID\", \"snapshot_date\").toPandas()\n",
    "\n",
    "features_pdf = features_sdf.toPandas()\n",
    "features_pdf.drop(columns=[c for c in [\"customerID\", \"snapshot_date\"] if c in features_pdf.columns], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f361665-930e-47ec-b312-679ecd40cb2e",
   "metadata": {},
   "source": [
    "## preprocess data for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3b44009-0ae7-41ca-a073-28bcbc23c8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop identifiers \n",
    "features_pdf.drop(columns=[c for c in [\"customerID\", \"snapshot_date\"] if c in features_pdf.columns], inplace=True)\n",
    "\n",
    "# Recreate tenure groups\n",
    "def create_tenure_groups(df):\n",
    "    df = df.copy()\n",
    "    df[\"tenure_group\"] = pd.cut(\n",
    "        df[\"tenure\"],\n",
    "        bins=[0, 12, 24, 36, 48, 60, 72, np.inf],\n",
    "        labels=[\"0-1yr\", \"1-2yr\", \"2-3yr\", \"3-4yr\", \"4-5yr\", \"5-6yr\", \"6+yr\"]\n",
    "    )\n",
    "    return df\n",
    "\n",
    "features_pdf = create_tenure_groups(features_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4045799d-d9cb-404f-99b3-f63f5499318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "numerical_cols = features_pdf.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "categorical_cols = features_pdf.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "\n",
    "for col in numerical_cols:\n",
    "    median_val = features_pdf[col].median()\n",
    "    features_pdf[col] = features_pdf[col].fillna(median_val)\n",
    "\n",
    "for col in categorical_cols:\n",
    "    mode_val = features_pdf[col].mode()[0] if not features_pdf[col].mode().empty else \"Unknown\"\n",
    "    features_pdf[col] = features_pdf[col].fillna(mode_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a4362f-9dee-4838-a030-a74b88884b4f",
   "metadata": {},
   "source": [
    "## model prediction inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ec78728-99af-4b99-89db-ac7f14da5385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pipeline\n",
    "import pickle\n",
    "\n",
    "with open('model_artifacts/model_pipeline.pkl', 'rb') as f:\n",
    "    pipeline = pickle.load(f)\n",
    "\n",
    "preprocessor = pipeline['preprocessor']\n",
    "model = pipeline['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "517a5173-a9b5-44ac-b210-fdc8376949ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform and predict\n",
    "X_inference = preprocessor.transform(features_pdf)\n",
    "y_inference = model.predict(X_inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66bf1150-721f-43ca-bba9-622f9383ffac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   customerID snapshot_date                  model_name  model_predictions\n",
      "0  7590-VHVEG    2024-06-01  log_reg_churn_model.joblib                  0\n",
      "1  5575-GNVDE    2024-06-01  log_reg_churn_model.joblib                  0\n",
      "2  3668-QPYBK    2024-06-01  log_reg_churn_model.joblib                  0\n",
      "3  7795-CFOCW    2024-06-01  log_reg_churn_model.joblib                  0\n",
      "4  9237-HQITU    2024-06-01  log_reg_churn_model.joblib                  0\n",
      "Number of predictions: 6948\n"
     ]
    }
   ],
   "source": [
    "y_inference_pdf = id_cols_pdf.copy()\n",
    "y_inference_pdf[\"model_name\"] = config[\"model_name\"]\n",
    "y_inference_pdf[\"model_predictions\"] = y_inference\n",
    "\n",
    "print(y_inference_pdf.head())\n",
    "print(\"Number of predictions:\", len(y_inference_pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3fe2b8-4642-486d-aa3b-2d7703ad3d15",
   "metadata": {},
   "source": [
    "## save model inference to datamartgold table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62b6ea81-a97b-423a-bf95-4c8072088c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved inference results to: datamart/gold/model_predictions/log_reg_churn_model.jo/log_reg_churn_model.jo_predictions_2024_06_01.parquet\n"
     ]
    }
   ],
   "source": [
    "gold_directory = f\"datamart/gold/model_predictions/{config['model_name'][:-4]}/\"\n",
    "\n",
    "if not os.path.exists(gold_directory):\n",
    "    os.makedirs(gold_directory)\n",
    "\n",
    "partition_name = f\"{config['model_name'][:-4]}_predictions_{config['snapshot_date_str'].replace('-', '_')}.parquet\"\n",
    "filepath = os.path.join(gold_directory, partition_name)\n",
    "\n",
    "spark.createDataFrame(y_inference_pdf).write.mode(\"overwrite\").parquet(filepath)\n",
    "print(\"Saved inference results to:\", filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4947d9dc-98ac-421f-955c-2d4b741343d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved inference results to CSV: datamart/gold/model_predictions/log_reg_churn_model.jo/log_reg_churn_model.jo_predictions_2024_06_01.csv\n"
     ]
    }
   ],
   "source": [
    "# Save as CSV\n",
    "csv_filepath = filepath.replace(\".parquet\", \".csv\")  # Change the extension to .csv\n",
    "\n",
    "# Save the predictions as CSV using pandas \n",
    "y_inference_pdf.to_csv(csv_filepath, index=False)\n",
    "print(\"Saved inference results to CSV:\", csv_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c339e3cb-1826-49a0-ac73-cb381f85b033",
   "metadata": {},
   "source": [
    "## Check datamart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6475e133-3d7a-4d21-809d-7a6d734ec793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session \n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"lr_model_checker\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")  # Clean output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9182b2ec-8daa-461c-b301-2e3c0f0f9e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read Parquet files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row_count: 78040\n",
      "+----------+-------------+--------------------+-----------------+\n",
      "|customerID|snapshot_date|          model_name|model_predictions|\n",
      "+----------+-------------+--------------------+-----------------+\n",
      "|4526-ZJJTM|   2024-07-01|log_reg_churn_mod...|                0|\n",
      "|8384-FZBJK|   2024-07-01|log_reg_churn_mod...|                0|\n",
      "|3750-RNQKR|   2024-07-01|log_reg_churn_mod...|                0|\n",
      "|0962-CQPWQ|   2024-07-01|log_reg_churn_mod...|                0|\n",
      "|3096-YXENJ|   2024-07-01|log_reg_churn_mod...|                0|\n",
      "+----------+-------------+--------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"datamart/gold/model_predictions/\"\n",
    "\n",
    "# Recursively list all files in the subfolders of the folder_path\n",
    "files_list = glob.glob(os.path.join(folder_path, '**', '*'), recursive=True)\n",
    "\n",
    "# Filter out only Parquet and CSV files\n",
    "parquet_files = [f for f in files_list if f.endswith(\".parquet\")]\n",
    "csv_files = [f for f in files_list if f.endswith(\".csv\")]\n",
    "\n",
    "# Read the Parquet files if they exist\n",
    "if parquet_files:\n",
    "    df = spark.read.option(\"header\", \"true\").parquet(*parquet_files)\n",
    "    print(\"Read Parquet files\")\n",
    "elif csv_files:\n",
    "    # Read the CSV files if they exist\n",
    "    df = spark.read.option(\"header\", \"true\").csv(*csv_files)\n",
    "    print(\"Read CSV files\")\n",
    "else:\n",
    "    print(\"No valid Parquet or CSV files found.\")\n",
    "\n",
    "# Show row count and schema of the DataFrame\n",
    "print(\"row_count:\", df.count())\n",
    "df.show(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff19c58-94ad-4abb-984f-b082326c2871",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
